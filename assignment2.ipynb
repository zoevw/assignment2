{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling and Exploratory Data Analysis\n",
        "## Do Q1 and Q2, and one other question.\n",
        "`! git clone https://www.github.com/DS3001/assignment2`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "1. Read the abstract. What is this paper about?\n",
        "> This paper focuses on the concept of \"data tidying\" as a crucial aspect of data cleaning to prepare data for analysis.\n",
        "2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "> The \"tidy data standard\" aims to provide a structured, standardized way to organize values within a dataset. By adhering to this stand, initial data cleaning becomes more efficient because it avoids the need to start from scratch every time.  \n",
        "3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "> The phrase \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way\" suggests that tidy datasets share a standard structure, whereas messy datasets vary widely in their disorganization, similar to how families have common traits but messy datasets have unique issues. The sentence \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general\" conveys that in a specific dataset, identifying observations and variables is straightforward. However, creating a precise, universal definition for these terms across all datasets is challenging due to the diverse data types and structures encountered.\n",
        "\n",
        "4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        ">  Values are numbers (quantitative) or strings (qualitative), organized based on variables and observations. Variables group values that measure the same underlying attribute (e.g., height, temperature) across units. Each variable contains related values.Observations group all the values measured on the same unit (e.g., a person, a day) across different attributes or variables. Each observation consists of related values for a specific unit.\n",
        "\n",
        "5. How is \"Tidy Data\" defined in section 2.3?\n",
        ">   \"Tidy Data\" is defined by three princples: each variable is a column, each observation is a row, and each type of data forms a separate table.Tidy data simplifies variable extraction and facilitates analysis by providing a standardized structure\n",
        "\n",
        "6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        ">  The five most common problems with messy datasets are: column headers being values instead of variable names, multiple variables being stored in a single column, variables being stored in both rows and columns, multiple types of observational units being stored in the same table, and a single observational unit being stored in multiple tables. The data in Table 4 are messy because the income categories (e.g., \"<$10k\", \"$10-20k\") are used as column headers instead of being a variable. To tidy the dataset, the process of \"melting\" is employed, which involves transforming columns into rows, making the dataset more organized and suitable for analysis.\n",
        "\n",
        "7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "> Table 11 is considered messy because the \"element\" column is not a variable but rather stores the names of the actual variables (e.g., tmin, tmax).Table 12(a) is in a melted and tidy format because each row represents a day's weather data (e.g., temperature) for a specific weather station, year, and month.\n",
        "\n",
        "8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "> The \"chicklen-and-egg\" problem is that tidy data's usefulness is heavily reliant on tools designed for it, creating a challenge in improving workflow independently. Looking ahead, Wickham hopes for progress in data wrangling through incremental improvements in understanding tidy data and tool development. He encourages exploring alternative tidy data formulations for better storage and tool efficiency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.**\n",
        "\n",
        "  1. This paper focuses on the concept of \"data tidying\" as a crucial aspect of data cleaning to prepare data for analysis.\n",
        "\n",
        "  2. The \"tidy data standard\" aims to provide a structured, standardized way to organize values within a dataset. By adhering to this stand, initial data cleaning becomes more efficient because it avoids the need to start from scratch every time.  \n",
        "\n",
        "  3. The phrase \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way\" suggests that tidy datasets share a standard structure, whereas messy datasets vary widely in their disorganization, similar to how families have common traits but messy datasets have unique issues. The sentence \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general\" conveys that in a specific dataset, identifying observations and variables is straightforward. However, creating a precise, universal definition for these terms across all datasets is challenging due to the diverse data types and structures encountered.\n",
        "\n",
        "  4. Values are numbers (quantitative) or strings (qualitative), organized based on variables and observations. Variables group values that measure the same underlying attribute (e.g., height, temperature) across units. Each variable contains related values.Observations group all the values measured on the same unit (e.g., a person, a day) across different attributes or variables. Each observation consists of related values for a specific unit.\n",
        "\n",
        "  5. \"Tidy Data\" is defined by three princples: each variable is a column, each observation is a row, and each type of data forms a separate table.Tidy data simplifies variable extraction and facilitates analysis by providing a standardized structure\n",
        "\n",
        "  6. The five most common problems with messy datasets are: column headers being values instead of variable names, multiple variables being stored in a single column, variables being stored in both rows and columns, multiple types of observational units being stored in the same table, and a single observational unit being stored in multiple tables. The data in Table 4 are messy because the income categories (e.g., \"<$10k\", \"$10-20k\") are used as column headers instead of being a variable. To tidy the dataset, the process of \"melting\" is employed, which involves transforming columns into rows, making the dataset more organized and suitable for analysis.\n",
        "\n",
        "  7. Table 11 is considered messy because the \"element\" column is not a variable but rather stores the names of the actual variables (e.g., tmin, tmax).Table 12(a) is in a melted and tidy format because each row represents a day's weather data (e.g., temperature) for a specific weather station, year, and month.\n",
        "\n",
        "  8. The \"chicklen-and-egg\" problem is that tidy data's usefulness is heavily reliant on tools designed for it, creating a challenge in improving workflow independently. Looking ahead, Wickham hopes for progress in data wrangling through incremental improvements in understanding tidy data and tool development. He encourages exploring alternative tidy data formulations for better storage and tool efficiency.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2kqA41pQFPbp"
      },
      "id": "2kqA41pQFPbp"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXHg7F7LKbL6"
      },
      "id": "CXHg7F7LKbL6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "\n",
        "> Using `data.info()`, I saw that the data type for `Price` was an object. This means that it probably contains non-numeric characters like dollar signs or commas, preventing it from being recognized as a numerical data type (float64). I cleaned the `Price` variable by removing special characters/commas and then converting it a numeric data type.\n",
        "\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://www.github.com/DS3001/assignment2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmBgaSpVHPa1",
        "outputId": "d0681900-070d-4141-9524-02b75e67dc25"
      },
      "id": "mmBgaSpVHPa1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assignment2'...\n",
            "warning: redirecting to https://github.com/DS3001/assignment2.git/\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 36 (delta 8), reused 5 (delta 5), pack-reused 24\u001b[K\n",
            "Receiving objects: 100% (36/36), 5.47 MiB | 21.78 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the data, print relevant info\n",
        "data = pd.read_csv('./assignment2/data/airbnb_hw.csv')\n",
        "print(data.info())\n",
        "\n",
        "# remove special characters and commas\n",
        "data['Price'] = data['Price'].replace('[\\$,]', '', regex=True)\n",
        "\n",
        "# convert to numeric\n",
        "data['Price'] = data['Price'].astype(float)\n",
        "\n",
        "# count missing values in the Price variable\n",
        "missing_values = data['Price'].isnull().sum()\n",
        "print(\"Missing values in Price:\", missing_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQz7xEm6HYui",
        "outputId": "38c6aec6-6570-43e9-d9cf-d51d2058a48a"
      },
      "id": "wQz7xEm6HYui",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30478 entries, 0 to 30477\n",
            "Data columns (total 13 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Host Id                     30478 non-null  int64  \n",
            " 1   Host Since                  30475 non-null  object \n",
            " 2   Name                        30478 non-null  object \n",
            " 3   Neighbourhood               30478 non-null  object \n",
            " 4   Property Type               30475 non-null  object \n",
            " 5   Review Scores Rating (bin)  22155 non-null  float64\n",
            " 6   Room Type                   30478 non-null  object \n",
            " 7   Zipcode                     30344 non-null  float64\n",
            " 8   Beds                        30393 non-null  float64\n",
            " 9   Number of Records           30478 non-null  int64  \n",
            " 10  Number Of Reviews           30478 non-null  int64  \n",
            " 11  Price                       30478 non-null  object \n",
            " 12  Review Scores Rating        22155 non-null  float64\n",
            "dtypes: float64(4), int64(3), object(6)\n",
            "memory usage: 3.0+ MB\n",
            "None\n",
            "Missing values in Price: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q5.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "> The Census Bureau collects race data according to U.S. Office of Management and Budget guidelines, and these data are based on self-identification. People may choose to report more than one race group, and people of any race may be of any ethnic origin.\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "> Data on race is important for various reasons including policy-making, addressing disparities, and ensuring equal opportunities. In politics and society, these data play a significant role in shaping policies, legislative districts, and resource allocation. Data quality is important because it ensures that the information used to make key  decisions is reliable, accurate, and complete.\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "> I think the Census performed well in collecting race data by utilizing a detailed race and ethnicity questionnaire, allowing individuals to identify with multiple racial categories. However, it could enhance inclusivity by providing additional options for specific ethnic groups and addressing underrepresentation of certain communities.Future large-scale surveys should adapt by regularly updating and expanding race categories to better capture the diversity of the population. Additionally, the Census's approach to self-identification and the ability to select multiple options can serve as a good practice to be adopted more widely in data collection efforts.\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "> Similar to race, the Census collects data on sex and gender through self-identification on the questionnaire. While often used interchangeably, sex and gender are two distinct concepts. Sex is based on biological attributes of males and females (e.g., chromosomes, anatomy, and hormones), while gender is a social construction whereby a society or culture assigns certain tendencies or behaviors to the concepts of masculinity and femininity. It is important to  improve rhe wording of these questions to ensure inclusivity and accuracy.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "> Cleaning data related to protected characteristics requires extreme caution to avoid perpetuating biases or violating privacy, while ensuring compliance with ethical standards. Missing values pose significant challenges such as skewed analysis and reinforcing disparities if not handled properly. Good practices involve employing anonymization, cautious imputation methods, and ethical data handling. On the contrary, bad practices may include biased imputation that exacerbates existing prejudices or inappropriate use of sensitive data without consent.\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?\n",
        "> Imputing values for protected characteristics raises concerns about privacy, fairness, accuracy, potential biases, and reinforcing stereotypes. It would be important to promote algorithmic transparency, compliance with laws, and context sensitivity to ensure responsible handling of sensitive attributes.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}